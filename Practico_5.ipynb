{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Practico_3y4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benotti/diplodatos2020/blob/master/Practico_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYWMIumxlaT-",
        "colab_type": "text"
      },
      "source": [
        "# Introducción al aprendizaje automático y aprendizaje supervisado\n",
        "\n",
        "\n",
        "En estos prácticos se espera que puedan poner en práctica los conocimientos adquiridos en los cursos Introducción al Aprendizaje Automático y Aprendizaje Supervisado, trabajando con el conjunto de datos de la mentoría. La metodología de seguimiento y entrega será la misma que para los prácticos anteriores. La fecha final de entrega será el 13/9, se harán entregas parciales pre-acordadas.\n",
        "\n",
        "El objetivo es que se apliquen el esquema aprendido para hacer tareas de aprendizaje automático: selección de un modelo, ajuste de hiperparámetros y evaluación. En el conjunto de datos tendrán que identificar claramente cuál es el atributo de etiqueta o clase y describirlo en su informe.\n",
        "\n",
        "Se espera que hagan uso de las herramientas vistas en los cursos. Se espera que hagan uso especialmente de las herramientas brindadas por `scikit-learn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-ckwJiClaUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Agregar las librerías que hagan falta\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUvq3_gvlaUS",
        "colab_type": "text"
      },
      "source": [
        "## Carga de datos y división en entrenamiento y evaluación\n",
        "\n",
        "La celda siguiente se encarga de la carga de datos. Uds deberán decidir cómo definir las etiquetas. Se sugiere trabajar con etiquetas binarias. Antes de modificar las etiquetas se solicita que cuenten cuántos diálogos caen en cada etiquetas. Se sugiere unificar las etiquetas negativas, las positivas y descartar los diálogos que tengan calificación neutra. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb2cii_olaUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv #COMPLETAR, charlar que comparen el split que se consigue por el siguiente método con el que tienen ya hecho.\n",
        "\n",
        "# División entre instancias y etiquetas\n",
        "X, y = dataset.iloc[:, 1:], dataset.TARGET\n",
        "\n",
        "# división entre entrenamiento y evaluación\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOrJv7-3laUf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Documentación:\n",
        "\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG6wGKfXlaUh",
        "colab_type": "text"
      },
      "source": [
        "## Ejercicio 1: Descripción de los Datos y la Tarea\n",
        "\n",
        "Responder las siguientes preguntas:\n",
        "\n",
        "1. ¿De qué se trata el conjunto de datos?\n",
        "2. ¿Cuál es la variable objetivo que hay que predecir? ¿Qué significado tiene? ¿Cómo la definieron de forma binaria?\n",
        "3. ¿Qué información (atributos) hay disponibles para hacer la predicción?\n",
        "4. ¿Qué atributos imagina ud. que pueden ayudar en la predicción?\n",
        "\n",
        "**No es necesario escribir código para responder estas preguntas. En los ejercicios 2 y 3 no trabajaremos con lenguaje natural**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLqvTgYRFqEM",
        "colab_type": "text"
      },
      "source": [
        "###[Desarrollo del ejercicio 1.](Practico_3y4_desarrollo/Practico_3y4_part_1.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVO-h8yZIH_a",
        "colab_type": "text"
      },
      "source": [
        "###[Generación del CSV para utilizarlo en los ejercicios 2 y 3.](Practico_3y4_desarrollo/Practico_3y4_part_2y3_preparacion.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gg2_ms4laUk",
        "colab_type": "text"
      },
      "source": [
        "## Ejercicio 2: Predicción con Modelos Lineales\n",
        "\n",
        "En este ejercicio se entrenarán modelos lineales de clasificación para predecir la variable objetivo.\n",
        "\n",
        "Para ello, deberán utilizar la clase SGDClassifier de scikit-learn.\n",
        "\n",
        "Documentación:\n",
        "- https://scikit-learn.org/stable/modules/sgd.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJOOa4grlaUn",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 2.1: SGDClassifier con hiperparámetros por defecto\n",
        "\n",
        "Entrenar y evaluar el clasificador SGDClassifier usando los valores por omisión de scikit-learn para todos los parámetros. Únicamente **fijar la semilla aleatoria** para hacer repetible el experimento.\n",
        "\n",
        "Evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1\n",
        "- matriz de confusión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p68R-cf2laUp",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 2.2: Ajuste de Hiperparámetros\n",
        "\n",
        "Seleccionar valores para los hiperparámetros principales del SGDClassifier. Como mínimo, probar diferentes funciones de loss, tasas de entrenamiento y tasas de regularización.\n",
        "\n",
        "Para ello, usar grid-search y 5-fold cross-validation sobre el conjunto de entrenamiento para explorar muchas combinaciones posibles de valores.\n",
        "\n",
        "Reportar accuracy promedio y varianza para todas las configuraciones.\n",
        "\n",
        "Para la mejor configuración encontrada, evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1\n",
        "- matriz de confusión\n",
        "\n",
        "Documentación:\n",
        "- https://scikit-learn.org/stable/modules/grid_search.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwveA8luJRre",
        "colab_type": "text"
      },
      "source": [
        "###[Desarrollo del ejercicio 2.](Practico_3y4_desarrollo/Practico_3y4_part_2.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY_iJ95flaUr",
        "colab_type": "text"
      },
      "source": [
        "## Ejercicio 3: Árboles de Decisión (optativo) \n",
        "\n",
        "En este ejercicio se entrenarán árboles de decisión para predecir la variable objetivo.\n",
        "\n",
        "Para ello, deberán utilizar la clase DecisionTreeClassifier de scikit-learn.\n",
        "\n",
        "Documentación:\n",
        "- https://scikit-learn.org/stable/modules/tree.html\n",
        "  - https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "- https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4Im2zidlaUt",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 3.1: DecisionTreeClassifier con hiperparámetros por defecto\n",
        "\n",
        "Entrenar y evaluar el clasificador DecisionTreeClassifier usando los valores por omisión de scikit-learn para todos los parámetros. Únicamente **fijar la semilla aleatoria** para hacer repetible el experimento.\n",
        "\n",
        "Evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1\n",
        "- matriz de confusión\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv_8PmgrlaUv",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 3.2: Ajuste de Hiperparámetros\n",
        "\n",
        "Seleccionar valores para los hiperparámetros principales del DecisionTreeClassifier. Como mínimo, probar diferentes criterios de partición (criterion), profundidad máxima del árbol (max_depth), y cantidad mínima de samples por hoja (min_samples_leaf).\n",
        "\n",
        "Para ello, usar grid-search y 5-fold cross-validation sobre el conjunto de entrenamiento para explorar muchas combinaciones posibles de valores.\n",
        "\n",
        "Reportar accuracy promedio y varianza para todas las configuraciones.\n",
        "\n",
        "Para la mejor configuración encontrada, evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1\n",
        "- matriz de confusión\n",
        "\n",
        "\n",
        "Documentación:\n",
        "- https://scikit-learn.org/stable/modules/grid_search.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVXHVODVJket",
        "colab_type": "text"
      },
      "source": [
        "###[Desarrollo del ejercicio 3.](Practico_3y4_desarrollo/Practico_3y4_part_3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGO5wPT3laUx",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio 3.3: Inspección del Modelo\n",
        "\n",
        "Si esto no lo hicieron creo que se puede sacar CHARLAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qFLEap-laU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dhVtLKwqYQu",
        "colab_type": "text"
      },
      "source": [
        "## Desde el procesamiento del lenguaje natural\n",
        "\n",
        "Hasta aquí trabajamos sólo con variables numéricas o categóricas. Empiecen a leer el siguiente tutorial para lo que viene. \n",
        "\n",
        "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "\n",
        "### Ejercicio 4 de preparación: Balance de clases\n",
        "\n",
        "Queremos analizar el balance de nuestras clases. Para eso, analizamos cuántos diálogos caen en cada clase que predicen nuestros modelos. Enunciar claramente cuáles son las clases y si están o no balanceadas. \n",
        "\n",
        "Si la distribución es muy desigual resulta dificil aplicar técnicas como el subsampling porque nos quedaríamos con muy pocos datos de entrenamiento, o el oversampling porque para que las clases queden parejas, deberíamos repetir un mismo diálogo muchas veces.\n",
        "\n",
        "Si la distribución no es tan desigual, hacer subsampling de diálogos completos y guardarlos como un nuevo dataset. A partir de ahora, todos los experimentos que corran deberán correrlos también para este nuevo corpus con sus clases balanceadas. Luego vamos a comparar los resultados obtenidos con y sin balanceo de clases\n",
        "\n",
        "###[Desarrollo del ejercicio 4.](Practico_3y4_desarrollo/Practico_3y4_part_4.ipynb)\n",
        "\n",
        "### Ejercicio 5: De palabras a números\n",
        "\n",
        "Los algoritmos de aprendizaje automático trabajan con espacios vectoriales (es decir, con muchos números). Entonces al trabajar con Procesamiento de Lenguaje Natural se plantea la cuestión de cómo representar texto con números. Hay muchas maneras de hacer esto y es un campo que sigue evolucionando con el tiempo. Una opción muy básica es asignarle a cada palabra que aparece en nuestro dataset un número según el orden en el que aparecen. Luego, una oración es un vector de índices de esas palabras. Pero esto tiene varios problemas. Entre ellos, los algoritmos de aprendizaje automático también requieren que los vectores tengan una longitud fija, con lo cual hay que recortar la oración o agregarle ceros al final (es decir, hacer padding). Por eso un enfoque clásico para representar texto es el Bag Of Words: un vector de bits del tamaño de todo nuestro vocabulario que tiene un uno si la palabra está en la oración y un 0 si no está. Este enfoque también se conoce como One Hot Vector. \n",
        "\n",
        "Referencias: \n",
        "1. https://es.wikipedia.org/wiki/Modelo_bolsa_de_palabras \n",
        "\n",
        "2. https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
        "\n",
        "3. https://towardsdatascience.com/the-magic-behind-embedding-models-part-1-974d539f21fd\n",
        "\n",
        "Para la implementación podrían usar Spacy, como se usó para la tokenización. Sin embargo creo que va a ser más sencillo para uds usar la libreria CountVectorizer que implementa el Bag Of Words de forma eficiente considerando que es una representación esparsa. Esta sugerencia es porque uds ya vienen usando sklearn.\n",
        "\n",
        "Con el objetivo de visualizar lo que hace CountVectorizer vamos a explorar sus hiperparámetros min_df, max_df y ngram-range. Observar cómo cambia el tamaño del vector.\n",
        "\n",
        "Los parámetros min_df y max_df que se corresponden con min y max document frequency. Ámbos toman valores entre 0 y 1 y estipulan el rango de frecuencia de aparición de una palabra dentro de un documento que vamos a aceptar. Es decir, si min_df es 0.005, todas las palabras que representen menos de un 0,5% de las palabras totales serán descartadas. Por el otro lado, si max_df es 0.35, todas las palabras que representen más de un 35% del total de palabras serán descartadas. Visualizar las palabras que serían descartadas dentro de este rango y describir cómo son. \n",
        "\n",
        "El parámetro ngram_range calcula la frecuencia de ngramas. Resulta útil para descubrir frases o expresiones comunes (además de las palabras comunes). Además, en combinación con el parámetro \"analyzer\" se pueden usar como ngramas de palabras o de caracteres.\n",
        "\n",
        "Referencias: \n",
        "4. https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "5. De palabras a vectores por DOT CVS: https://www.youtube.com/watch?v=Tg1MjMIVArc\n",
        "\n",
        "\n",
        "NOTA: Como el tamaño del vector (es decir, el vocabulario) debe ser igual para el entrenamiento como para el test y el dev, tenemos que vectorizar todos los dataset de la misma forma. \n",
        "\n",
        "\n",
        "###[Desarrollo del ejercicio 5.](Practico_3y4_desarrollo/Practico_3y4_part_5.ipynb)\n",
        "\n",
        "### Ejercicio 6: Menos puede ser más?\n",
        "\n",
        "En este ejercicio explorarán el uso de data ablations. Eliminar parte de los datos para hacer predicciones en el ejercicio 8. Qué pasa si sólo usan el último 20%/40%/60% de los turnos de los diálogos? Qué pasa si sólo usan los turnos del estudiante? O sólo los del tutor? Siempre reportar el tamaño del train del val y del test para cada ablation. En el ejercicio 8 reportar sobre todas las ablations. Qué pasa si sólo usamos los emoticones? Visualizar los emoticones más frecuentes para los diálogos positivos y negativos. \n",
        "\n",
        "###[Desarrollo del ejercicio 6.](Practico_3y4_desarrollo/Practico_3y4_part_6.ipynb)\n",
        "\n",
        "### (Opcional, sólo lectura) Ejercicio 7: Word embeddings densos\n",
        "\n",
        "Las Bag Of Words tienen un problema importante: no preservan el orden de las palabras dentro de una oración (si uno trabaja con unigramas, y los ngramas capturan orden de forma limitada) ni la relación semántica de las palabras entre sí. Este problema dio lugar a otros enfoques como los word embeddings densos. \n",
        "\n",
        "Los embeddings densos son algo bastante nuevo en el campo del Procesamiento Del Lenguaje Natural (ultimos 10 años) pero fueron algo totalmente revolucionario. Desde que aparecieron las primeras versiones de embeddings (word2vec, glove, varias otras) surgieron muchas versiones distintas hechas con diversos algoritmos y técnicas. Pero todos tienen algo en común: tratan de captar la semántica de una palabra representandola con un vector (una lista de números) que se calcula en base a los valores de las palabras que aparecen alrededor de esa palabra (se suele decir que aparecen en el contexto de la palabra pero en mi opinión esta es una visión muy restringida de contecto). O sea, para cada palabra se calcula de manera iterativa un valor sobre la base de qué palabras aparecen antes y después en miles y miles de textos que se usan para entrenar los embeddings. Esos valores luego se exportan y se usan como representación de las palabras.\n",
        "\n",
        "No es del alcance de este trabajo práctico meterse en este tema, que correspondería más a un curso introductorio de Procesamiento del Lenguaje Natural y ya no a Machine Learning, pero me pareció interesante comentarselos como una alternativa (muy muy) frecuente frente al problema de decidir como representar texto con números.\n",
        "\n",
        "Si les interesa y quieren leer/investigar más al respecto, aca hay dos referencias. La primera es un video corto de difusión que da las intuiciones pero no entra en ningún detalle. La segunda es una clase de Stanford del curso de Chris Manning que tiene bastante matemática. \n",
        "\n",
        "Referencias: \n",
        "6. Word embeddings densos por DOT CVS: https://www.youtube.com/watch?v=RkYuH_K7Fx4\n",
        "7. Una clase del curso de Stanford: https://www.youtube.com/watch?v=ERibwqs9p38&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=2\n",
        "\n",
        "\n",
        "### Ejercicio 8: \n",
        "\n",
        "Elegir 2 o 3 clasificadores (pueden ser los que usamos al principio de este práctico. Reportar las mismas métricas que al principio del práctico. \n",
        "\n",
        "###[Desarrollo del ejercicio 8a.](Practico_3y4_desarrollo/Practico_3y4_part_8a.ipynb)\n",
        "\n",
        "\n",
        "###[Desarrollo del ejercicio 8b.](Practico_3y4_desarrollo/Practico_3y4_part_8b.ipynb)\n",
        "\n",
        "\n",
        "###[Desarrollo del ejercicio 8c.](Practico_3y4_desarrollo/Practico_3y4_part_8c.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WPc-YG5sXnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}